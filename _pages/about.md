---
permalink: /
title: <p align="center">Haiquan Lu（卢海泉）</p>
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
  
Biography
------
Hi! I am a final-yeal Computer Science Undergraduate student at [Nankai University](https://www.nankai.edu.cn/main.htm), where I am fortunately advised by Prof. [Ming-Ming Cheng](https://mmcheng.net/cmm/comment-page-1/) and [Xialei Liu](https://mmcheng.net/xliu/) in [Media Computing Lab (VCIP)](https://mmcheng.net). I am working closely with Prof. [Yaoqing Yang](https://sites.google.com/site/yangyaoqingcmu/) at [Dartmouth College](https://home.dartmouth.edu) as a research intern, and with Prof. [Michael Mahoney](https://nicholas.carlini.com) at [UC Berkeley](https://www.berkeley.edu).

My research focuses on **Mechine Learning** and its real-world applications. I am currently working on improving **efficiency, transparency, and robustness** of mechine learning models, utilizing high-dimension features such as [loss landscapes](https://sites.google.com/view/hidimlearning23/home?authuser=0), [weight matrix analysis](https://sites.google.com/view/heavy-tails-ml-2023). This research contributes to [AlphaPruning](https://arxiv.org/pdf/2410.10912) and [SharpBalance](https://arxiv.org/pdf/2407.12996).

I’m actively seeking a PhD program in Computer Science for the current cycle and would love to connect about potential opportunities.


Selected Publications
------
+ **AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models** <br/>  **Haiquan Lu**, *Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, Yaoqing Yang* <br/> [![](https://img.shields.io/badge/NeurIPS-2024-e1dd72)](https://openreview.net/forum?id=fHq4x2YXVv&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)) [![](https://img.shields.io/badge/Paper-a9cce3)](https://www.arxiv.org/pdf/2410.10912) [![](https://img.shields.io/badge/Website-bf2c34)](https://neurips.cc/virtual/2024/poster/94217) [![](https://img.shields.io/badge/Poster-f47a60)](https://neurips.cc/media/PosterPDFs/NeurIPS%202024/94217.png?t=1733538065.8645315) [![](https://img.shields.io/badge/Slides-097770)](https://docs.google.com/presentation/d/1LZlIC7l2Ysod0SVQb9xi0PU0FZC23sBQBzULLXIFo7U/edit?usp=sharing) [![](https://img.shields.io/badge/Code-4d5198)](https://github.com/haiquanlu/AlphaPruning) <br/> **AlphaPruning** proposed an approach to analyze model layer quality, automatically allocate compression factors (such as sparsity or quantization precision) to transformer layers, applicable to LLaMA, OPT, Mistral, ViT, and ConvNext. Pruning LLaMA-7B to 80% sparsity leads to 3.06× end-to-end speedup on CPUs. It
also improves the structured/semi-structured pruning and mix-precision quantization.
+ **Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance** <br/> **Haiquan Lu**, *Xiaotian Liu, Yefan Zhou, Qunli Li, Kurt Keutzer, Michael W. Mahoney, Yujun Yan, Huanrui Yang, Yaoqing Yang* <br/> [![](https://img.shields.io/badge/NeurIPS-2024-e1dd72)](https://openreview.net/forum?id=wJaCsnT9UE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)) [![](https://img.shields.io/badge/Paper-a9cce3)](https://arxiv.org/pdf/2407.12996) [![](https://img.shields.io/badge/Website-bf2c34)](https://neurips.cc/virtual/2024/poster/93160) [![](https://img.shields.io/badge/Poster-f47a60)](https://neurips.cc/media/PosterPDFs/NeurIPS%202024/93160.png?t=1733545349.459817) [![](https://img.shields.io/badge/Slides-097770)](https://docs.google.com/presentation/d/1o2clZCjMDsPlr3bIy14wSojsx0lMR71OpLNk5yhWIc8/edit?usp=sharing) [![](https://img.shields.io/badge/Code-4d5198)](https://github.com/haiquanlu/SharpBalance) <br/> We discover **sharpness-diversity trade-off**: minimizing the sharpness in the loss landscape tends to diminish the diversity of members within the ensemble, adversely affecting the ensemble's improvement. We introduce a new ensemble training method called **SharpBalance** to address it.
+ **Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation** <br/> *Zhengyuan Xie, **Haiquan Lu**, Jia-wen Xiao, Enguang Wang, Le Zhang, Xialei Liu* <br/> [![](https://img.shields.io/badge/ECCV-2024-e1dd72)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03745.pdf) [![](https://img.shields.io/badge/Paper-a9cce3)](https://arxiv.org/pdf/2407.14142) [![](https://img.shields.io/badge/Website-bf2c34)](https://eccv.ecva.net/virtual/2024/poster/414) [![](https://img.shields.io/badge/Poster-f47a60)](https://neurips.cc/media/PosterPDFs/NeurIPS%202024/93160.png?t=1733545349.459817) [![](https://img.shields.io/badge/Vedio-097770)](https://youtu.be/-KH5ZiOyKrI) [![](https://img.shields.io/badge/Code-4d5198)](https://github.com/zhengyuan-xie/ECCV24_NeST) <br/> **Class incremental semantic segmentation** aims to preserve old knowledge while learning new tasks, however, impeded by catastrophic forgetting and background shift issues. Prior works indicate the pivotal importance of initializing new classifiers and mainly focus on transferring knowledge from the background classifier or preparing classifiers for future classes, neglecting the alignment and variance of new classifiers. In this paper, we propose a new classifier pre-tuning (**NeST**) method applied before the formal training process, learning a transformation from old classifiers to generate new classifiers for initialization rather than directly tuning the parameters of new classifiers.

<div style="height: 300px;"></div>

<!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=RDdNMEkPYOvHnn4Mr-0kAnakB8Z_o6113sJcvEWqA_4"></script> -->
